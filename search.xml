<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>二分查找的各种变形</title>
    <url>/2019/11/26/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E7%9A%84%E5%90%84%E7%A7%8D%E5%8F%98%E5%BD%A2/</url>
    <content><![CDATA[<h2 id="二分查找，寻找插入的位置"><a href="#二分查找，寻找插入的位置" class="headerlink" title="二分查找，寻找插入的位置"></a>二分查找，寻找插入的位置</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> l, <span class="keyword">int</span> h, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//开区间</span></span><br><span class="line">    <span class="keyword">while</span> (l &lt; h) &#123;</span><br><span class="line">        <span class="keyword">int</span> m = l + (h - l) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">if</span> (nums[m] == k) &#123;</span><br><span class="line">            <span class="keyword">return</span> m;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nums[m] &gt; k) &#123;</span><br><span class="line">            h = m;</span><br><span class="line">        &#125; <span class="keyword">else</span></span><br><span class="line">            l = m + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> l;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>算法和数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title>Scala和Java的版本兼容问题</title>
    <url>/2019/11/26/Scala-and-java-version-compatibility-issues/</url>
    <content><![CDATA[<p><img src="https://github.com/IBITM/ibitm.github.io/tree/master/img/1574740536160.png" alt="图片来自Java 版本兼容性问题"></p>
<p><a href="https://docs.scala-lang.org/overviews/jdk-compatibility/overview.html" target="_blank" rel="noopener">JDK COMPATIBILITY</a></p>
]]></content>
      <tags>
        <tag>Scala</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 关于闭包的若干思考</title>
    <url>/2019/11/26/Spark-%E5%85%B3%E4%BA%8E%E9%97%AD%E5%8C%85%E7%9A%84%E8%8B%A5%E5%B9%B2%E6%80%9D%E8%80%83/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Spark 共享变量的若干思考</title>
    <url>/2019/11/26/reflections-on-Spark-shared-variables/</url>
    <content><![CDATA[<p>通常在向 Spark 传递函数时，可以使用 driver programme 中定义的变量，但是集群中运行的每个 tusk 都只会得到这些变量的一份新的副本，更新这些副本的值也不会影响driver programme 中的对应变量。</p>
<p>Spark 的两个共享变量，累加器与广播变量，使用结果聚合与广播这两种常见的通信模式突破了这一限制。</p>
<a id="more"></a>
<h2 id="Accumulator"><a href="#Accumulator" class="headerlink" title="Accumulator"></a>Accumulator</h2><h3 id="基本使用方法"><a href="#基本使用方法" class="headerlink" title="基本使用方法"></a>基本使用方法</h3><p>第一种共享变量，即累加器，提供了将 worker node 中的值聚合到 driver programme 中的简单语法。</p>
<p>下面的代码展示了一个 accumulator 被用于对一个数组中的元素求和:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> accum = sc.longAccumulator(<span class="string">"My Accumulator"</span>)</span><br><span class="line">accum: org.apache.spark.util.<span class="type">LongAccumulator</span> = <span class="type">LongAccumulator</span>(id: <span class="number">0</span>, name: <span class="type">Some</span>(<span class="type">My</span> <span class="type">Accumulator</span>), value: <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; accum.add(x))</span><br><span class="line">...</span><br><span class="line"><span class="number">10</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">18</span>:<span class="number">41</span>:<span class="number">08</span> <span class="type">INFO</span> <span class="type">SparkContext</span>: <span class="type">Tasks</span> finished in <span class="number">0.317106</span> s</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res2: <span class="type">Long</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>可以通过调用 <code>SparkContext.longAccumulator()</code> 或 <code>SparkContext.doubleAccumulator()</code> 方法创建数值类型的 <code>accumulator</code>（累加器）以分别累加 Long 或 Double 类型的值。集群上正在运行的任务就可以使用 <code>add</code> 方法来累计数值。</p>
<p>注意，worker node 上的 tusk 不能访问累加器的值。从这些任务的角度来看，累加器是一个只写变量。只有 driver program 才可以使用 <code>value</code> 方法读取累加器的值。</p>
<h3 id="累加器与容错"><a href="#累加器与容错" class="headerlink" title="累加器与容错"></a>累加器与容错</h3><p>Spark 会自动重新执行失败的或较慢的仕务来应对有错的或着比较慢的机器。例如，如果对某分区执行 <code>map()</code> 操作的节点失败了，Spark 会在另一个节点上重新运行该任务。即使该节点没有崩溃，而只是处理速度比别的节点慢很多，Spark 也可以抢占式地在另一个节点上启动一个“投机”（speculative）型的任务副本，如果该任务更早结束就可以直接获取结果。即使没有节点失败，Spark有时也需要重新运行任务来获取缓存中被移除出内存的数据。</p>
<p><em>因此最终结果就是同一个函数可能对同一个数据运行了多次。</em></p>
<p>这种情况下累加器要怎么处理呢？</p>
<p>实际结果是，对于要在 action 操作中使用的累加器，Spark只会把每个任务对累加器的修改应用一次。</p>
<p>因此，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在 <code>foreach()</code> 这样的 action 操作中。对于在 RDD  transform 操作中使用的累加器，就不能保证有这种情况了。transform 操作中累加器可能会发生不止一次更新。</p>
<p>举个例子，当一个被缓存下来但是没有经常使用的RDD在第一次从 LRU 缓存中被移除并又被重新用到时，这种非预期的累加器的多次更新就会发生。</p>
<h2 id="Broadcast-Variables"><a href="#Broadcast-Variables" class="headerlink" title="Broadcast Variables"></a>Broadcast Variables</h2><p>Spark 的第二种共享变量是<em>广播变量</em>，它可以让程序高效的向所有的 worker node 发送一个较大的 只读值。比如，你的应用需要向所有的节点发送一个较大的只读查询表，甚至是机器学习算法中的一个很大的特征向量，这个时候广播变量就会很好用了。</p>
<p>如果你对闭包有了解，你会发现，Spark 会自动把闭包中所有引用到的变量发送到 worker node上。这虽然很方便，但也很低效。原因有二：</p>
<ul>
<li>默认的任务发送机制是专门为小任务进行优化的，当发送的共享数据量较大时效率较低</li>
<li>事实上你可能会在多个操作中使用同一个变量，但是Spark会为每个操作分别发送。</li>
</ul>
<p>广播变量通过在一个变量 <code>v</code> 上调用 <code>SparkContext.broadcast(v)</code> 方法来进行创建。广播变量是 <code>v</code> 的一个 wrapper（包装器），可以通过调用 <code>value</code> 方法来访问它的值。代码示例如下:</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这里的 v 指的是 Array(1, 2, 3)</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows下的Spark程序开发环境配置</title>
    <url>/2019/11/25/Spark-program-development-environment-configuration-under-Windows/</url>
    <content><![CDATA[<p>在Windows环境下, 使用IDEA进行Spark程序开发所需的环境配置, 主要是Maven pom文件的配置和Scala SDK的配置。</p>
<a id="more"></a>
<h3 id="Maven的安装与配置"><a href="#Maven的安装与配置" class="headerlink" title="Maven的安装与配置"></a>Maven的安装与配置</h3><p>需要配置<code>Maven home directory</code>、<code>User settings file</code>、<code>Local repository</code>、 <code>Import Maven projects automatically</code></p>
<h2 id="IDEA-Maven工程创建与配置"><a href="#IDEA-Maven工程创建与配置" class="headerlink" title="IDEA Maven工程创建与配置"></a>IDEA Maven工程创建与配置</h2><h3 id="配置pom-xml文件"><a href="#配置pom-xml文件" class="headerlink" title="配置pom.xml文件"></a>配置<code>pom.xml</code>文件</h3><p><strong>spark精简版的pom.xml</strong></p>
<p><a href="https://github.com/apache/spark/blob/master/examples/pom.xml" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/examples/pom.xml</a></p>
<p><strong>spark最全版的pom.xml</strong></p>
<p><a href="https://github.com/apache/spark/blob/master/pom.xml" target="_blank" rel="noopener">https://github.com/apache/spark/blob/master/pom.xml</a></p>
<p><strong>maven repository</strong></p>
<p><a href="https://mvnrepository.com/search?q=spark" target="_blank" rel="noopener">https://mvnrepository.com/search?q=spark</a></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span> <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span> <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>war<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>TestSpark<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.kfk.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>TestSpark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.12<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scala.binary.version</span>&gt;</span>2.11<span class="tag">&lt;/<span class="name">scala.binary.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="导入Scala-SDK"><a href="#导入Scala-SDK" class="headerlink" title="导入Scala SDK"></a>导入<code>Scala SDK</code></h3><p><code>groupId</code>和<code>artifactId</code>被统称为“坐标”是为了保证项目唯一性而提出的</p>
<p><code>groupId</code>一般分为多个段，这里我只说两段，第一段为域，第二段为公司名称。举个apache公司的tomcat项目例子：这个项目的<code>groupId</code>是org.apache</p>
<p>artifactId表示这个项目的名称</p>
<p>包结构最好是<code>groupId.artifactId.xxx</code></p>
<h2 id="编写spark程序"><a href="#编写spark程序" class="headerlink" title="编写spark程序"></a>编写spark程序</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.spark.test</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyScalaWordCout</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//参数检查</span></span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: MyWordCout    "</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//获取参数</span></span><br><span class="line">    <span class="keyword">val</span> input=args(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> output=args(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">//创建scala版本的SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"myWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//读取数据</span></span><br><span class="line">    <span class="keyword">val</span> lines=sc.textFile(input)</span><br><span class="line">    <span class="comment">//进行相关计算</span></span><br><span class="line">    <span class="keyword">val</span> resultRdd=lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    <span class="comment">//保存结果</span></span><br><span class="line">    resultRdd.saveAsTextFile(output)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="打成jar包然后上传到服务器"><a href="#打成jar包然后上传到服务器" class="headerlink" title="打成jar包然后上传到服务器"></a>打成jar包然后上传到服务器</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/spark-submit --master <span class="built_in">local</span>[2] /opt/jars/sparkStu.jar hdfs://bigdata-pro01.kfk.com:9000/user/data/stu.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure>

<p>Some of the commonly used options are:</p>
<blockquote>
<ul>
<li><code>--class</code>: The entry point for your application (e.g. <code>org.apache.spark.examples.SparkPi</code>)</li>
<li><code>--master</code>: The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="noopener">master URL</a> for the cluster (e.g. <code>spark://23.195.26.187:7077</code>)</li>
<li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>) (default: <code>client</code>) <strong>†</strong></li>
<li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown).</li>
<li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an <code>hdfs://</code> path or a <code>file://</code> path that is present on all nodes.</li>
<li><code>application-arguments</code>: Arguments passed to the main method of your main class, if any</li>
</ul>
</blockquote>
]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>hello world</title>
    <url>/2019/11/25/first-blog/</url>
    <content><![CDATA[<h2 id="第一篇博客"><a href="#第一篇博客" class="headerlink" title="第一篇博客"></a>第一篇博客</h2><p>昨天花了一些时间搭建了 hexo 的框架，打算慢慢的从知乎转移到个人博客了。</p>
<p>为什么想要写博客呢，一方面是想要记录自己的成长，看着自己学会、总结的东西越来越多，还是很有成就感的；另一方面是想要留下点东西，帮助别人吧，把自己踩过的坑，走过的弯路以及经验分享给别人吧。</p>
<p>写博客是一个慢活，得精雕细琢，这对于我这个急性子是一个挺大的挑战。希望写博客能帮我客服一下这个毛病吧。</p>
<p>最后的最后希望，我能一直坚持下去，不断的输入和不断的输出。</p>
]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
